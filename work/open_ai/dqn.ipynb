{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN（deep Q-learning）でCartPoleを学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: http://neuro-educator.com/rl2/\n",
    "# ref: https://qiita.com/yukiB/items/0a3faa759ca5561e12f8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque \n",
    "\n",
    "# gym\n",
    "import gym\n",
    "\n",
    "# plot\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    " # deep learning\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_config\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "# original\n",
    "import utils.display as disp\n",
    "import utils.learning as learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Huber関数\n",
    "# err < ϵ の範囲では2次関数だが、その外側の範囲では線形に増加する\n",
    "# 外れ値の影響を受けにくい\n",
    "# ref: http://ibisforest.org/index.php?Huber%E9%96%A2%E6%95%B0\n",
    "def huberloss(y_true, y_pred):\n",
    "    err = y_true - y_pred\n",
    "    cond = K.abs(err) < 1.0\n",
    "    L2 = 0.5 * K.square(err)\n",
    "    L1 = (K.abs(err) - 0.5)\n",
    "    loss = tf.where(cond, L2, L1) # Kerasではバッチ内の施行ごとに損失関数を変えられない\n",
    "    return K.mean(loss)\n",
    "\n",
    "# Deep Q Learning\n",
    "class DQN:\n",
    "    def __init__(self, learning_rate=0.01, gamma=0.99, state_size=4, action_size=2, hidden_size=10):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        # モデル作成\n",
    "        # relu\n",
    "        # 勾配消失/爆発しない。隠れ層のみに使用する\n",
    "        # ref: https://qiita.com/miyamotok0105/items/3435930cc04650bce54d\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(hidden_size, activation='relu', input_dim=state_size))\n",
    "        self.model.add(Dense(hidden_size, activation='relu'))\n",
    "        self.model.add(Dense(action_size, activation='linear'))\n",
    "        self.optimizer = Adam(lr=learning_rate)\n",
    "        self.model.compile(loss=huberloss, optimizer=self.optimizer)\n",
    "        \n",
    "    # ミニバッチ確率的勾配降下法で学習する\n",
    "    # ref: http://shironeko.hateblo.jp/entry/2016/10/29/173634\n",
    "    # Experience Replay\n",
    "    # 逐次学習では無く、メモリに保持しておいて後で学習する\n",
    "    def replay(self, memory, batch_size, targetQN):\n",
    "        inputs = np.zeros((batch_size, self.state_size)) # 入力\n",
    "        targets = np.zeros((batch_size, self.action_size)) # 出力\n",
    "        # 使用する行動履歴をランダムサンプリング\n",
    "        # Experience Replay\n",
    "        mini_batch = memory.sample(batch_size)\n",
    "        # 学習データを作成する\n",
    "        for i, (state_b, action_b, reward_b, next_state_b) in enumerate(mini_batch):\n",
    "            inputs[i:i+1] = state_b #  ステート\n",
    "            target = reward_b # 報酬\n",
    "            \n",
    "            if  not (next_state_b == np.zeros(state_b.shape)).all(axis=1): # 最後のステップでは無い場合\n",
    "                # 次回の最大評価値\n",
    "                remainQs = self.model.predict(next_state_b)[0]\n",
    "                next_action = np.argmax(remainQs)\n",
    "                # 報酬 + 学習率 * 次回の最大評価値\n",
    "                target = reward_b + self.gamma * targetQN.model.predict(next_state_b)[0][next_action]\n",
    "            \n",
    "            # 学習で近づける評価値\n",
    "            targets[i] = self.model.predict(state_b)\n",
    "            targets[i][action_b] = target   \n",
    "            \n",
    "        # 学習させる\n",
    "        self.model.fit(inputs, targets, epochs=1, verbose=0)\n",
    "            \n",
    "    def clone_model(self, custom_objects={}):\n",
    "        config = {\n",
    "            'class_name': self.model.__class__.__name__,\n",
    "            'config': self.model.get_config(),\n",
    "        }\n",
    "        clone = model_from_config(config, custom_objects=custom_objects)\n",
    "        clone.set_weights(self.model.get_weights())\n",
    "        return clone\n",
    "\n",
    "    # モデルを可視化\n",
    "    def plot_model(self):\n",
    "        plot_model(self.model, to_file='qnetwork.jpg', show_shapes=True)\n",
    "        plt.figure(figsize = (6,6))\n",
    "        plt.imshow(np.asarray(Image.open('qnetwork.jpg')))        \n",
    "\n",
    "# 行動履歴\n",
    "class History:\n",
    "    def __init__(self, max_size=100):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.buffer)\n",
    "                \n",
    "# ε-グリーディ法で行動を選択\n",
    "def get_action(state, episode, targetQN, is_learning):\n",
    "    epsilon = 0.001 + 0.9 / (1.0+episode)\n",
    "    if not is_learning or epsilon <= np.random.uniform(0, 1):\n",
    "        retTargetQs = targetQN.model.predict(state)[0]\n",
    "        action = np.argmax(retTargetQs) # 最大評価の行動\n",
    "    else:\n",
    "        action = np.random.choice([0, 1])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "\n",
    "dqn_mode = False # DQN or DDQN\n",
    "num_max_episode = 5000 # 学習する最大エピソード回数\n",
    "num_max_step = 200 # 1エピソードの最大ステップ数\n",
    "num_action = env.action_space.n # アクション数\n",
    "num_state = observation.shape[0] # ステート数\n",
    "num_goal_avg_episode = 100 # 学習を終了させる平均計算をするエピソード数\n",
    "goal_avg_rewaed = 195 # 学習を終了させる平均報酬\n",
    "\n",
    "num_batch = 32 #  学習バッチサイズ\n",
    "num_history  = 10000 # 保存する最大行動履歴数\n",
    "num_hidden_unit = 16 # 隠れ層のユニット数\n",
    "learning_rate = 0.0025 # QNの学習係数\n",
    "gamma = 0.99  # QNの割引係数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習させるメインのQN\n",
    "mainQN = DQN(\n",
    "    state_size=num_state, \n",
    "    action_size=num_action, \n",
    "    hidden_size=num_hidden_unit, \n",
    "    learning_rate=learning_rate,\n",
    "    gamma=gamma)\n",
    "\n",
    "# 学習中に予測を行うQN（一つ前にエピソードのQN）\n",
    "# Fixed Target Q-Network\n",
    "# 少し前に固定しておいてQNを学習中の予測に使用する\n",
    "targetQN = DQN(\n",
    "    state_size=num_state, \n",
    "    action_size=num_action, \n",
    "    hidden_size=num_hidden_unit, \n",
    "    learning_rate=learning_rate,\n",
    "    gamma=gamma)\n",
    "\n",
    "mainQN.plot_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History(max_size=num_history)\n",
    "rewards = []\n",
    "rewards_goal_eval = np.zeros(num_goal_avg_episode)\n",
    "\n",
    "for episode in range(num_max_episode):\n",
    "    episode_reward = 0\n",
    "    observation = env.reset()\n",
    "    state = np.reshape(observation, [1, 4])\n",
    "    targetQN = mainQN.clone_model() # 一つ前のエピソードのQN\n",
    "    \n",
    "    for step in range(num_max_step):\n",
    "        action = get_action(state, episode, targetQN, is_learning=True)\n",
    "        next_observation, reward, done, _ =  env.step(action)\n",
    "        next_state = np.reshape(next_observation, [1, 4])\n",
    "        \n",
    "        # 各ステップで貰える報酬を-1, 0, 1で固定\n",
    "        # clipping \n",
    "        # ゲーム内容によらず、同じハイパーパラメータのQNを使用できる\n",
    "        if done: # 終了\n",
    "            next_state = np.zeros(state.shape)\n",
    "            if step < num_max_step-5:\n",
    "                reward = -1 # 失敗\n",
    "            else:\n",
    "                reward = 1 # 成功\n",
    "        else:\n",
    "            reward = 0 # プレイ中\n",
    "            \n",
    "        episode_reward = episode_reward + 1\n",
    "        \n",
    "        history.add((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "        \n",
    "        if history.len() > num_batch: # バッチ数まで溜まった場合\n",
    "            # 学習\n",
    "            mainQN.replay(history, num_batch, targetQN)\n",
    "        \n",
    "        # DQNモードなら最新のQNを使用させる\n",
    "        if dqn_mode:\n",
    "            targetQN = mainQN\n",
    "            \n",
    "        if done:\n",
    "            rewards.append(episode_reward)\n",
    "            rewards_goal_eval = np.hstack((rewards_goal_eval[1:], episode_reward))\n",
    "            break\n",
    "            \n",
    "    # ゴール達成のため学習終了\n",
    "    reward_avg = rewards_goal_eval.mean()\n",
    "    print('episode: {}, episode_reward: {}'.format(episode, reward_avg))\n",
    "    if reward_avg >= goal_avg_rewaed:\n",
    "        print('learning finished: {}'.format(episode))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習後のQNでシュミレーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "frames = []\n",
    "for episode in range(num_max_step):\n",
    "    state = np.reshape(observation, [1, 4])\n",
    "    action = get_action(state, episode, mainQN,  is_learning=False)\n",
    "    observation, reward, done, _ =  env.step(action)\n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "disp.display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # エピソード毎の獲得報酬を移動平均線で表示\n",
    "df_rewards = pd.DataFrame({'num': range(0, len(rewards)), 'reward': rewards})\n",
    "df_rewards['reward_rolling_mean_10'] = df_rewards['reward'].rolling(window=10,center=False).mean()\n",
    "df_rewards.plot(kind='line', x='num', y='reward_rolling_mean_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
